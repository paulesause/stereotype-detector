# Racism classifier

This Project classifies paragraphs news articles according to the stereotype content model employing a BERT classifiert.

## Setup
### Clone Repository

Clone the repository by running

```bash
git clone https://github.com/paulesause/racism_classifier
```

### Install Dependencies

1. Create virtualenvironment

```bash
virtualenv venv
```

2.Activate virtualenvironment 

```bash
source venv/bin/activate
```

3. Install dependencies from requirements.txt file

```bash
pip install -r requirements.txt
```

4. Install local racism_classifier package in editable mode

```
pip install -e .
```
### Data

Create a `data` folder in the root of the repository. Place a `.xlsx` file with your labled data in this `data` folder.
The `.xlsx` sould contain the following columns:

- **`text_block`**: Containing the new acticle paragraphs
- **`warm`**: labled either 1 if the group of interst is described as warm or 0 if not
- **`cold`**: labled either 1 if the group of interst is described as cold or 0 if not 

### Environment Variables

Setup the following environment variables in `.env` file at the root of the repository.

1. Huggingface Repository

```
HUGGING_FACE_BERT_MODEL_REPRO=<hugging-face-user-name/model-repository-name>
```

2. Huggingface Access Token

```
HUGGING_FACE_TOKEN=<your-huggingface-access-token>
```

# Usage
## Finetune BERT Model

Finetuning a BERT Classifier can be done with the **`finetune(...)`** method in the **`racism_classifier/finetuning/BERT.py`** module while the hyperparameter ranges are defined in `racism_classifier/config.py`.

To perfom a fintuning follow this steps:

1. Define hyperparameter ranges in `racism_classifier/config.py` as described below.
2. Use `finetune(...)` method as described below

### Hyperparameter Ranges

Hyperparameter Ranges are defined in the `racism_classifier.config.HYPERPARAMETER_SPACE` dictionary. `Optuna` uses the specifications in this dictionary to propose hyperparameters for the next trail in the `racism_classifier.hyperparameter_optimization.optuna_hp_space_BERT(...)` method.

An example initialisation is:

```python
HYPERPARAMETER_SPACE = {
    "learning_rate": {"type": "float", "low": 1e-6, "high": 1e-4, "log": True},
    "per_device_train_batch_size": {"type": "categorical", "choices": [1, 2, 3, 4]},
    "num_train_epochs": {"type": "int", "low": 1, "high": 2}
}
```

### `finetune(...)`

This method performs end-to-end fine-tuning of a BERT-based classification model using Hugging Face's `transformers` library, with support for both **holdout evaluation** and **cross-validation (CV)** strategies. It includes **hyperparameter tuning with Optuna**, pushes the best model to the Hugging Face Hub, and logs metrics during training.

---

#### Parameters

- **`model`** (`str`):  
  Name or path of the pre-trained Hugging Face model (e.g., `"bert-base-uncased"`).  **Must be specified.**

- **`output_dir`** (`str`):  
  Path to save training logs, checkpoints, and model outputs.  **Must be specified.**

- **`hub_model_id`** (`str`):  
  Name of the model repository on Hugging Face Hub. **Must be specified.**

- **`evaluation_mode`** (`str`, default=`"holdout"`):  
  Strategy to evaluate model performance.  
  Options:
  - `"holdout"`: Use a standard train/validation/test split.
  - `"cv"`: Use stratified K-fold cross-validation with final evaluation on test set.

- **`n_example_sample`** (`int`, optional):  
  If set, randomly sample the first `n` examples for faster debugging or development.

---

#### Workflow Overview

- **Authentication**  
  Logs into Hugging Face Hub using a personal token.

- **Data Loading & Preprocessing**
  - Loads and optionally samples dataset.
  - Rescales multi-label columns (e.g., `cold`, `warm`).
  - Tokenizes the text and removes unused columns.

- **Hyperparameter Tuning**
  - If `evaluation_mode == "holdout"`:  
    Runs Optuna tuning on a train/validation split using Hugging Faceâ€™s `Trainer.hyperparameter_search`.
  - If `evaluation_mode == "cv"`:  
    Runs cross-validation with `optuna.Study.optimize()` using custom objective logic.

- **Training Final Model**
  - Trains the best model using the full training set and best hyperparameters.

- **Evaluation & Logging**
  - Evaluates on a holdout test set.
  - Logs metrics.

- **Push to Hub**
  - Uploads the trained model and tokenizer to the Hugging Face Hub.

---

#### Raises

- `AssertionError`: If `hub_model_id` is not provided or is not a string.  
- `ValueError`: If `evaluation_mode` is not `"holdout"` or `"cv"`.

### Example Usage

This script train a BERT classifiers in different variants.

```python
from racism_classifier.finetuning import BERT

# small data set
# holdout
print("""
# small data set
# holdout
      """)
BERT.finetune(
    model="distilbert-base-uncased",
    hub_model_id="<your-hugging-face-name>/distilbert-base-uncased_holdout",
    evaluation_mode="holdout",
    output_dir="models/BERT_holdout",
    n_example_sample=20
)

# cv
print("# cv")

BERT.finetune(
    model="distilbert-base-uncased",
    hub_model_id="<your-hugging-face-name>/distilbert-base-uncased_cv",
    evaluation_mode="cv",
    output_dir="models/BERT_cv",
    n_example_sample=20
)

# large data set
print("# large data set")
BERT.finetune(
    model="distilbert-base-uncased",
    hub_model_id="<your-hugging-face-name>/distilbert-base-uncased_entire_data",
    evaluation_mode="holdout",
    output_dir="models/BERT_entire_data"
)
```